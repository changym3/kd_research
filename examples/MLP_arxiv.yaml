# python run_experiments.py -c MLP.yaml -nc "trainer.gpu=1" "model.aug.k=0"


meta:
  model_name: MLP
  dataset_name: ogbn-arxiv
  n_runs: 1

dataset:
  num_features: ~
  num_classes: ~

model:
  num_hiddens: 256
  num_layers: 3
  dropout: 0.5
  input_dropout: 0.1
  batch_norm: True
  aug:
    k: 5
    combine: cat # cat
    norm: True

trainer:
  lr: 0.001
  weight_decay: 0
  epochs: 1000
  verbose: True
  # ckpt_dir: ./examples/ckpt/test_GAT
  gpu: 1
